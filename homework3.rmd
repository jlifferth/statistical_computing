---
title: 'Bios 6301: Assignment 3'
author: "Jonathan Lifferth"
output:
  pdf_document: default
  html_document: default
---

*Due Tuesday, 26 September, 1:00 PM*

50 points total.

Add your name as `author` to the file's metadata section.

Submit a single knitr file (named `homework3.rmd`) by email to marisa.h.blackman@vanderbilt.edu.
Place your R code in between the appropriate chunks for each question.
Check your output by using the `Knit HTML` button in RStudio.

$5^{n=day}$ points taken off for each day late.

### Question 1 ###

**15 points**

Write a simulation to calculate the power for the following study
design.  The study has two variables, treatment group and outcome.
There are two treatment groups (0, 1) and they should be assigned
randomly with equal probability.  The outcome should be a random normal
variable with a mean of 60 and standard deviation of 20.  If a patient
is in the treatment group, add 5 to the outcome.  5 is the true
treatment effect.  Create a linear model for the outcome by the
treatment group, and extract the p-value (hint: see assignment1).
Test if the p-value is less than or equal to the alpha level, which
should be set to 0.05.

Repeat this procedure 1000 times. The power is calculated by finding
the percentage of times the p-value is less than or equal to the alpha
level.  Use the `set.seed` command so that the professor can reproduce
your results.

1. Find the power when the sample size is 100 patients. (10 points)

```{r}
library(MASS)
?mvrnorm()
set.seed(1234)
# sample size for each of the two categories (so this number is doubled in the simulation)
sample_size <- 50
alpha <- 0.05
below_alpha <- list()

# I know that it would be better to also incorporate a binomial function to randomly determine the treatment group for each specific value, but I'm not sure how to also integrate that function with the overall distribution with mean = 60 and sd = 20, therefore, I simply created two separate groups of equal size, which should approximate two categories with equal chance of occurrence.
for (i in seq(1:1000)) {
    df0 <- data.frame(value = rnorm(sample_size, mean = 60, sd = 20), treatment = rep(0, sample_size))
    df1 <- data.frame(value = rnorm(sample_size, mean = 60, sd = 20) + 5, treatment = rep(1, sample_size))

    df_combined <- rbind(df0, df1)
    model <- lm(value ~ treatment, dat=df_combined)
    below_alpha[i] <- summary(model)$coefficients[,4][2] < alpha
}

below_alpha_unlisted <- unlist(below_alpha)
power = sum(below_alpha_unlisted) / length(below_alpha_unlisted)
power
```


1. Find the power when the sample size is 1000 patients. (5 points)
```{r}
set.seed(1234)
# sample size for each of the two categories (so this number is doubled in the simulation)
sample_size <- 500
alpha <- 0.05
below_alpha <- list()


# I know that it would be better to also incorporate a binomial function to randomly determine the treatment group for each specific value, but I'm not sure how to also integrate that function with the overall distribution with mean = 60 and sd = 20, therefore, I simply created two separate groups of equal size, which should approximate two categories with equal chance of occurrence.
for (i in seq(1:1000)) {
    df0 <- data.frame(value = rnorm(sample_size, mean = 60, sd = 20), treatment = rep(0, sample_size))
    df1 <- data.frame(value = rnorm(sample_size, mean = 60, sd = 20) + 5, treatment = rep(1, sample_size))

    df_combined <- rbind(df0, df1)
    model <- lm(value ~ treatment, dat=df_combined)
    below_alpha[i] <- summary(model)$coefficients[,4][2] < alpha
}

below_alpha_unlisted <- unlist(below_alpha)
power = sum(below_alpha_unlisted) / length(below_alpha_unlisted)
power
```


### Question 2 ###

**14 points**

Obtain a copy of the [football-values lecture](https://github.com/couthcommander/football-values).
Save the `2023/proj_wr23.csv` file in your working directory.  Read
in the data set and remove the first two columns.

1. Show the correlation matrix of this data set. (4 points)

```{r}

football_df <- read.csv('proj_wr23.csv')
football_df <- subset(football_df, select = -c(PlayerName, Team))
football_correlation <- cor(football_df)
football_correlation
```


1. Generate a data set with 30 rows that has a similar correlation
structure.  Repeat the procedure 1,000 times and return the mean
correlation matrix. (10 points)
```{r}

vcov.football=var(football_df)
means.football=colMeans(football_df)

correlation_matrices <- list()

for (i in (1:1000)) {
   football.sim = mvrnorm(30, mu = means.football, Sigma = vcov.football)
   football.sim = as.data.frame(football.sim)
   rho.sim=cor(football.sim)   ## Simulated correlation matrix
   correlation_matrices[[i]] <- rho.sim
}

mean_matrix <- matrix(0, nrow = nrow(football_correlation), ncol=ncol(football_correlation))
for (matrix_ in correlation_matrices) {
    mean_matrix <- mean_matrix + matrix_
}

mean_matrix <- mean_matrix / length(correlation_matrices)
mean_matrix
```



### Question 3 ###

**21 points**

Here's some code:

```{r}
nDist <- function(n = 100) {
    df <- 10
    prob <- 1/3
    shape <- 1
    size <- 16
    list(
        beta = rbeta(n, shape1 = 5, shape2 = 45),
        binomial = rbinom(n, size, prob),
        chisquared = rchisq(n, df),
        exponential = rexp(n),
        f = rf(n, df1 = 11, df2 = 17),
        gamma = rgamma(n, shape),
        geometric = rgeom(n, prob),
        hypergeometric = rhyper(n, m = 50, n = 100, k = 8),
        lognormal = rlnorm(n),
        negbinomial = rnbinom(n, size, prob),
        normal = rnorm(n),
        poisson = rpois(n, lambda = 25),
        t = rt(n, df),
        uniform = runif(n),
        weibull = rweibull(n, shape)
    )
}
```

1. What does this do? (3 points)

    ```{r}
    round(sapply(nDist(500), mean), 2)
    ```
    
    ```
    answer here
    The nDist function takes a number "n" and calculates a series of summary statistics for 
    various types of distributions of size "n". The sapply((_, mean), 2) then takes the mean
    of the values from each distribution. 
    ```

1. What about this? (3 points)

    ```{r}
    sort(apply(replicate(20, round(sapply(nDist(10000), mean), 2)), 1, sd))
    ```
    
    ```
    answer here
    Here, the mean for each distribution is calculated (as above) but the results are rounded (to the nearest whole (1) digit) and then the results are replicated 20 times *and* each distribution type is sorted in ascending order of values.
    ```

    In the output above, a small value would indicate that `N=10,000` would provide a sufficent sample size as to estimate the mean of the distribution. Let's say that a value *less than 0.02* is "close enough".

1. For each distribution, estimate the sample size required to simulate the distribution's mean. (15 points)
```{r}

apply(replicate(20, round(sapply(nDist(2000), mean), 2)), 1, sd)


```


Don't worry about being exact. It should already be clear that N < 10,000 for many of the distributions. You don't have to show your work. Put your answer to the right of the vertical bars (`|`) below.

distribution|N
---|---
beta| 6
binomial| 10000
chisquared| 50000
exponential|4000
f| 1300
gamma|2000
geometric| 10000
hypergeometric| 3800
lognormal| 10000
negbinomial| 30000
normal| 3100
poisson| 59000
t| 4700
uniform| 300
weibull| 2000
